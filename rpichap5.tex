\chapter{Pilot Study Results And Analysis} \label{sec:results}

\section{Participant Background Feedback}

In the two week timespan that OASIS was publicly available 57 users registered and participated in the pilot user study.
% We are going to have to cite these reddit and facebook
I advertised OASIS on social media outlets such as Facebook and Reddit.
Both of these social media outlets have a wide range of users with varying experiences.
Moreover, one of the online outlets I advertised OASIS on is unofficially affiliated with Rensselaer Polytechnic Institute (RPI).
Figure-\ref{} demonstrates the affiliation of users who registered to use OASIS for this pilot study.
As shown in Figure-\ref{} a majority of participants, that provided their affiliation, are actually not affiliated with RPI. 
This is a big change from previous user studies that only used RPI affiliated participants.
Additionally data shows that majority of participants, who are affiliated with RPI, are undergraduate.
It is particularly interesting that the major of overall participants did not provide information on their affiliation with RPI.
Specifically 65\% of participants did not provide feedback on their affiliation, and not all participants who claimed to be affiliated with RPI specified how they were affiliated;
About 2\% of participants have unknown affiliations with RPI.
The difference between RPI affiliated participants to non-RPI affiliated participants could be a direct results of how and when we advertised OASIS.
Particularly, during this pilot user study we advertised towards non-RPI affiliated outlets first and  RPI affiliated outlets last.
Additionally we asked our participants concerning their experience with architecture and visual arts.
Figure-\ref{} shows the distribution of participants' formal education and job experience in both architecture and visual arts.
A majority of our participants expressed that they have no experience with any of the related fields. 
Also those few participants that do have experience, have had only 1-4 years of exposure to formal architecture education or form visual arts education.
However, there is a user registered on OASIS that claims to have over 10 years of job experience in architecture.

Aside from asking about architecture and visual arts experience, I also let participants elaborate on other relevant experiences.
Some of our participants have had experience in civil engineering, electrical engineering, studio arts, user experience design, and architectural engineering with focus on lighting.
Furthermore, I also asked participants to provide a list of 3D modeling softwares they have had exposure to.
As seen in Figure-\ref{} participants have had the most experience with AudoCad\cite{} and SketchUp{}. 
A few participants have had experience w/ 3dsmax\cite{} and Maya\cite{}.
Again, we let participants elaborate on their experience with other 3D modeling softwares. 
Some other 3D modeling softwares, not shown in Figure-\ref{}, that participants have had experience in include SoilidWorks\cite{}, AGI32\cite{}, Dialux\cite{}, and Daysim\cite{}.
Note that AGI32, Dialux, and Daysim are not specifically 3D modeling software but rather daylight analysis and performance tools.
From data collected on participants' affiliations, experience in related fields, and exposure to modeling software, I can support that OASIS seems accessible to a wide variety of users.

In addition to trying understand if OASIS is accessible, I also wanted feedback on the usability of our sketching interface.
Data on how participants spend time on OASIS can provide insight on user behavior.
Figure-\ref{} illustrates the distribution of participants in relation to their time spend on OASIS.
From Figure-\ref{} is it clear that the majority of users registered and participating in the pilot user study spent no time on the actual interface.
On the other hand, the average time spent per participant is about 12 minutes, excluding those participants that do not spend longer than a minute on OASIS past registration.
Although our user retention rate is low, I suspect that the voluntary nature, anonymity, and absence of renumeration in our pilot user study plays a significant role in the large number of participants who register and do not use OASIS.
Furthermore, we have no data on user participation in similar online user studies targeting similar social media outlets.
Equally important, Figure-\ref{} illustrates user time spend on OASIS per page.
Figure-\ref{} shows that participants spend 36\% of their time on the \textit{Sketch a Room} page.
Next participants spend 23\% of their time on the \textit{Create/Load Model} page of OASIS.
It is important to note that first-time users have the option of viewing a short tutorial video; the 1 minute long tutorial video coupled with the fact that the loading page is the first page users are directed to after logging in could directly contribute to the large portion of time users spend on the loading page.
Surprisingly, the page participants spend the least amount of time is on the \textit{Analyze Simulation} page. 
On the \textit{Analyze Simulation} page user would view daylight renderings of user designed models.
On average participants spent only 8\% of their time analyzing their designed models;
When compared to the 25\% of time participants spend on viewing 3D interpretation of their sketches on the \textit{Generate 3D model} page, the time spend on the \textit{Analyze Simulation} page seems remarkably low.
The difference between time spend on these two pages could stem from the fact that the \textit{Analyze Simulation} page is the final page new woulds would visit when navigating OASIS linearly.
Furthermore all of the temporal data collected on pages in OASIS could be effected by time spend filling out feedback questions, leaving OASIS running in the background as participants focus on other task, and users leaving OASIS before creating a single rendering.

\section{Usability Feedback}

Most of the feedback collected on the usability of OASIS is qualitative in nature.
We collect this qualitative data to get a clearer picture of how users perceive OASIS.
Quantitative 'yes' and 'no' feedback would not fully capture how participants experience our sketching interface.
Table-\ref{} list all feedback collected in regards to what participants found fun or interesting in our sketching environment.
Table-\ref{} is based off 14 participants that answered this feedback question.
Overall 6 of the participants mentioned that the interface was either fun or easy-to-use.
However, some participants found window placement non-intuitive and other participants had difficulty with the limited primitives we provide.
While, we did not explicitly ask what they found difficult difficult in the interface in this specific feedback questions, their response will be taken into consideration.
The participant who found window placement difficult states that they tried to leave a gap between walls to define where to place windows.
I suspect that the participant must have skipped the tutorial video or not consulted the many help options on OASIS.
Moreover, the other participant concerned with the limited options on OASIS might be comparing our tool to other more fully featured modeling softwares.
Interestingly our only participant with 10+ years of architectural job experience stated that the sketching interface was "very simple" and that that they never encountered modeled that could not be interpreted.
Other feedback comments find specific features interesting such as furniture items, skylights, and the daylight simulations.

% Grammar check me
We also reached out to participants to provide additional features we could add to our sketching interface to extend the flexibility of OASIS.
The two of the most common features requested by participants are the addition of doors on the sketching interface and a wider variety of furniture items.
Also, some users desired more control over primitives on the sketching interface. Including drag and drop mechanics on walls after initial placement and the precise manipulation of the dimensions of furniture items.
Additionally, our participant with 10+ years or architecture experience suggest control over window heights, ceiling heights, and window finishes.
Interestingly, an unanticipated situation with the participant feedback became clear analyzing the feedback for this question.
Some of our 14 users provided duplicate responses from previously asked questions.
Table-\ref{} displays  all responses collected that was not a duplicate response to a previous question.

In addition to collecting feedback on feature request, I also ask participants to describe some designs that they were unable to create due to system limitations.
Table-\ref{} shows participants feedback on limitations on design in our sketching interface.
The most common design limitation is the absence of doors in our sketching interface.
From  the feedback collected it seems that users assumed that they could not design multi-room sketches because of the lack of doors in the sketching interface.
In actuality, previous user studies have confirmed that the physical sketch interpretation algorithm can handle multi-room designs\cite{}.
Other limitations some users claimed to face was the lack of light shelves in our interface, the inability to place one piece of furniture on top of another such as in lofted bed, and absence of control over scale.
Participants also expressed that our selection of furniture items limited designs.
Overall, the main take away from user feedback concerning feature suggestions is that doors are essential to communicate to participants that our sketching interface supports multi room designs.
On a similar note Table-\ref{} list out user feedback regarding disliked elements of our sketching interface.
A common dislike in our sketching interface was the absence of scale.
Currently, we convey scale indirectly though statically sized furniture items, however feedback suggest that we make scale more explicit to users.
Interestingly, as users expressed dislike with our interface because we did not support keyboard shortcuts for common actions, such as undo.
Other dislikes with our sketching interface include, again, the limited collection of furniture we support, absence of doors,the inability to move walls and windows after initial placement, and the the lack of accurately when selecting a geographical locations for sketches.

Lastly, we asked users if there were any elements in our interface that were hard to use.
Feedback from that question can be seen on table-\ref{}.
Many users response to this question with stating nothing was hard to use.
However, there were few users experienced software bugs with the interface and reported them on this feedback question.
Aside from a few fixable software bugs, of which did not impact the entire system, a participant found the redundancy of Raphael FreeTransform handles confusing.
FreeTransform handles are three white circles that are overlaid onto furniture items when clicked.
One circle appears at the center of the furniture item, and the two other handles are paced perpendicularly some distance away from the furniture item.
As of now, these two perpendicular handles are used solely to rotate items.
Participant feedback on user interface elements help me note overlooked redundancies such as the two rotation FreeTransform handles.

To fully analyze the feedback collected from our sketching interface we must understand that omission of feedback could potentially be used to communicate feedback.
For example, when asked about negative aspects of our interface many users choose to respond with "no" or "none".
However, some users, whom readily provide feedback decide to omit feedback for specific questions.
I cannot assume that users imply there are no negative elements on our sketching interface based on user omission of negative feedback questions.
Improvements need to be made to remove ambiguity in omitting feedback.
Additionally, many responses given do not answer the question asked of users, rather users provide additional features that is more relevant to another question in our pilot user study.
Despite this, the sketching interface garnered overall positive feedback. 
Many participants claimed that the interface was easy to use and interesting.



\section{Model Based Feedback}

There are currently 73 models on OASIS and on average each user generates 1.25 models.
The distribution of the number of models made per user is illustrated in Figure-\ref{}.
From Figure-\ref{} we can see that most of our users only crated 1 model. A handful of users created more then 1 model.
However, currently there are 3 users that created more than 9+ models.
Moreover, while the number of models per users is relatively low, the number of renovations per models show that on average there are 1.9 renovations per model created.
Meaning that about half of our users renovate their models at least more than once.
Figure-\ref{} show the distribution of models and the number of renovations on these models.
Analyze of individual models is out of the scope of this thesis, however, I do display some user created models in Figure-\ref{}.
After creating a 3D model users can voluntarily provide feedback as to if we watch their intentions in interpreting their sketches.
I hypothesized that as models grew more complex the accuracy of our interpretation algorithm would decrease.
Figure-\ref{} illustrates model complexity against matching user intentions.
Model complexity was simply calculated as the sum of the number of primitives in a sketch.
Figure-\ref{} is interesting because models regardless of complexity seem to always match users initial intentions without requiring renovations.
Even models with 20 to 29 primitives seem to always match.
While the data is strongly indicative that our physical sketch interpretation algorithm is accurate, I believe that more feedback is required before any statically significant conclusions can be drawn.
From Figure-\ref{} and the fact that there are 73 models on OASIS, but only N responses, it is clear users do not answer this feedback question readily.

Aside from categorical quantitative feedback, we also ask users to quantitatively describe their overall impressions of the system effectiveness in construction 3D models from user sketches; 
Table-\ref{} displays results from this feedback questions.
Of the 16 users that provided feedback on the effectiveness of the physical sketch interpretation algorithm, 14 stated that the system matched their intentions.
One user stated that they only saw 2D versions of the interpretation.
This could be caused from the user either not rotating their model or limited WebGL support on their web browser.
I do  not collect meta data on user web browsers so there currently no explanation of this.
Two users mentioned the absence of support for doors hindered the effectiveness of generating 3D models.
One user linked us an image in their feedback, that displayed a rendering issue.
The detail of some of the qualitative feedback provided from users was much higher than originally expected.
Equally important, we asked users to describe cases where models were incorrectly interpreted by our physical sketching algorithm.
Table-\ref{} presents the feedback given about these failure cases.


































